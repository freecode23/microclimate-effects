{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "# graph\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics\n",
    "\n",
    "#  for multicolinearity\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Import Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Save csv files as pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --> NO NEED TO RUN SAVED AS PICKLE FILES <--\n",
    "# WEATHER FILES ##\n",
    "\n",
    "# AZ PHX Sky Harbor Data #\n",
    "AZW_15 = pd.read_csv(\"./Data/Weather Data/KPHX-15.csv\")\n",
    "\n",
    "# ENVIMET DATA #\n",
    "BPS = []\n",
    "Fname = []\n",
    "for path in pathlib.Path(\"./Data/BPS\").iterdir():\n",
    "    if path.is_file():\n",
    "        current_file = pd.read_csv(path)\n",
    "        BPS.append(current_file)\n",
    "        Fname.append(path.name.replace('.csv', ''))\n",
    "\n",
    "# CAMPUS METABOLISM DATA #\n",
    "metabol14 = []\n",
    "for path in pathlib.Path('./Data/ASU 2018').iterdir():\n",
    "    if path.is_file():\n",
    "        current_file = pd.read_csv(path)\n",
    "        metabol14.append(current_file)\n",
    "\n",
    "## Drop last row of EnviMet Data\n",
    "for i in range(len(BPS)):\n",
    "    BPS[i] = BPS[i].drop(16)\n",
    "\n",
    "## Save files as pickle\n",
    "AZW_15.to_pickle(\"AZW_15.pkl\")\n",
    "\n",
    "with open('BPS.pkl', 'wb') as f:\n",
    "    pickle.dump(BPS, f)\n",
    "\n",
    "with open('Fname.pkl', 'wb') as f:\n",
    "    pickle.dump(Fname, f)\n",
    "\n",
    "with open('metabol14.pkl', 'wb') as f:\n",
    "    pickle.dump(metabol14, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 All year data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_cols = ['Month', 'Time']\n",
    "\n",
    "dict_dtypes = {x : 'str'  for x in string_cols}\n",
    "\n",
    "# 1. all year data\n",
    "all_year = pd.read_csv('./Data/weather_st2', index_col = 0, dtype = dict_dtypes)\n",
    "j9 = pd.read_csv('./Data/weather_j9', index_col = 0, dtype = dict_dtypes)\n",
    "\n",
    "# 2. score data\n",
    "scores_df = pd.read_csv('./Data/score', index_col = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = all_year.drop(labels = ['CHWTON'], axis =1)\n",
    "Y = all_year['CHWTON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Train test split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state = 20)\n",
    "\n",
    "# 2. Train\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_RF = RandomForestRegressor(n_estimators = 100, random_state = 20)\n",
    "model_RF.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Tuning using RandomSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to set the parameter for our RF regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# 1. Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 50, stop = 300, num = 6)]\n",
    "\n",
    "# 2. Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(2, 15, num = 4)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# 3. Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# 4. Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [ 2, 5, 10]\n",
    "\n",
    "# 5. Method of selecting samples for training each tree\n",
    "bootstrap = [True]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "print(random_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf,\n",
    "                               param_distributions = random_grid,\n",
    "                               n_iter = 20, cv = 5,\n",
    "                               verbose = 2,\n",
    "                               scoring ='r2',\n",
    "                               random_state = 20,\n",
    "                               n_jobs = -1)\n",
    "\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print winning set of hyperparameters\n",
    "from pprint import pprint\n",
    "pprint(rf_random.best_estimator_.get_params())\n",
    "pprint(rf_random.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_best = rf_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # June 9th prediction\n",
    "# # 1. get X Y\n",
    "X_j9 = j9.drop(labels = ['CHWTON'], axis =1)\n",
    "Y_j9 = j9['CHWTON']\n",
    "\n",
    "# 2. Ypred\n",
    "Y_pred_random_j9 = rf_best.predict(X_j9)\n",
    "\n",
    "# 3. Score\n",
    "R2_j9_random = rf_best.score(X_j9, Y_j9)\n",
    "RMSE_j9_random = np.sqrt(metrics.mean_squared_error(Y_j9, Y_pred_random_j9))\n",
    "\n",
    "# 4. append to score df\n",
    "score_J9_random = [R2_j9_random, RMSE_j9_random]\n",
    "scores_df['RF_J9_AZ_random'] = score_J9_random\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. XG Boost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "all_year_dum = pd.read_csv('./Data/weather_st', index_col = 0, dtype = dict_dtypes)\n",
    "all_year_dum = all_year_dum.drop(labels = ['Hour_num', 'Month_num','Minute_num'], axis = 1)\n",
    "\n",
    "# 1. dummify dates and time\n",
    "all_year_dum = pd.get_dummies(all_year_dum, prefix=None, prefix_sep='_')\n",
    "\n",
    "# 2. get j9 data\n",
    "j9_dum = all_year_dum.iloc[15150:15246]\n",
    "\n",
    "# 3. concat j9_dum with j9 to remove non 0 minute\n",
    "j9_dum =  pd.concat([j9_dum, j9.drop(labels = ['Air Temp', 'Rel Humid', 'KW'], axis = 1)], axis = 1, join = \"inner\")\n",
    "\n",
    "# 3. drop June 9th data on original data\n",
    "all_year_dum = all_year_dum.drop(all_year_dum.index[15150:15246])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Get X and Y\n",
    "X_dum = all_year_dum.drop(labels = ['CHWTON'], axis =1)\n",
    "\n",
    "# 5. Train test split\n",
    "X_train_dum, X_test_dum, Y_train, Y_test = train_test_split(X_dum, Y, test_size=0.2, random_state = 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Randomized Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A parameter grid for XGBoost\n",
    "params = {\n",
    "    'n_estimators':[50, 100 , 250],\n",
    "    'min_child_weight':[4,5], \n",
    "    'gamma':[i/10.0 for i in range(3,6)],  \n",
    "    'subsample':[i/10.0 for i in range(6,11)],\n",
    "    'colsample_bytree':[i/10.0 for i in range(6,11)], \n",
    "    'max_depth': [2,3,4,6,7],\n",
    "    'objective': ['reg:squarederror', 'reg:tweedie'],\n",
    "    'booster': ['gbtree', 'gblinear'],\n",
    "    'eval_metric': ['rmse'],\n",
    "    'eta': [i/10.0 for i in range(3,6)],\n",
    "}\n",
    "\n",
    "reg = XGBRegressor()\n",
    "\n",
    "n_iter_search = 20\n",
    "xgb_random = RandomizedSearchCV(reg,\n",
    "                                param_distributions = params,\n",
    "                                n_iter = n_iter_search,\n",
    "                                cv = 5,\n",
    "                                verbose = 2,\n",
    "                                random_state = 20,\n",
    "                                scoring ='r2',\n",
    "                                n_jobs = -1)\n",
    "\n",
    "start = time.time()\n",
    "xgb_random.fit(X_train_dum, Y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time.time() - start), n_iter_search))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. print winning set of hyperparameters\n",
    "pprint(xgb_random.best_estimator_.get_params())\n",
    "pprint(xgb_random.best_score_)\n",
    "\n",
    "# 2. save model\n",
    "xgb_best = xgb_random.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# June 9th prediction\n",
    "# 1. get X\n",
    "X_j9_dum = j9_dum.drop(labels = ['CHWTON', 'Month', 'Time'], axis = 1)\n",
    "\n",
    "# 2. Ypred\n",
    "Y_pred_xgb_j9 = xgb_best.predict(X_j9_dum)\n",
    "\n",
    "# 3. Score\n",
    "R2_j9_xgb = xgb_best.score(X_j9_dum, Y_j9)\n",
    "RMSE_j9_xgb = np.sqrt(metrics.mean_squared_error(Y_j9, Y_pred_xgb_j9))\n",
    "\n",
    "# 4. append to score df\n",
    "score_J9_xgb = [R2_j9_xgb, RMSE_j9_xgb]\n",
    "scores_df['XGB_J9_AZ'] = score_J9_xgb\n",
    "scores_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
