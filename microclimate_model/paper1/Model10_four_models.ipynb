{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba628875-88b7-4b70-bbaa-4665018d58af",
   "metadata": {},
   "source": [
    "Use the final csv files for train and test and use various models to train and score them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3470fa-5ba9-4de4-8bbe-c6562636c324",
   "metadata": {},
   "source": [
    "# 0. Import and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8eec3d-cd80-40fd-8f52-fc1c339babbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import lightgbm\n",
    "\n",
    "# processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# parameters search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# scoring\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# explain\n",
    "import shap\n",
    "import datetime\n",
    "\n",
    "# save model\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Import constants\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d965fb-4a23-4961-b4e8-9f9f684f9877",
   "metadata": {},
   "source": [
    "# 1. Get the Train and Test Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df99023-b4f8-4af1-ad73-ee049b88002c",
   "metadata": {},
   "source": [
    "## 1.1 Combine the Three Buildings Data and Save as CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "241d3f20-4f8b-4126-8a70-d7080d0ffeee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading from....: ../data/dataset2/Psychology_North.csv\n",
      "loading from....: ../data/dataset2/Istb_4.csv\n",
      "loading from....: ../data/dataset2/Psychology.csv\n"
     ]
    }
   ],
   "source": [
    "# Function to load and add dummy variable\n",
    "def load_and_add_dummy(dir_path, file_name, dummy_name):\n",
    "    print(f\"loading from....: {dir_path}/{file_name}.csv\")\n",
    "    df = pd.read_csv(f\"{dir_path}/{file_name}.csv\")\n",
    "    df[dummy_name] = 1\n",
    "    return df\n",
    "\n",
    "def combine_csv(save_dir_name, save_file_name):\n",
    "    \"\"\"\n",
    "    Function to combine separate buildings csv into 1 csv and add extra 5 columns that sums each of the facade's\n",
    "    short and long wave radiation. \n",
    "    I\n",
    "    \"\"\"\n",
    "    # Load files and add dummy variables\n",
    "    psychology_north_df = load_and_add_dummy(save_dir_name, \"Psychology_North\", config.PSYCHOLOGY_NORTH)\n",
    "    istb_4_df = load_and_add_dummy(save_dir_name, \"Istb_4\", config.ISTB4)\n",
    "    psychology_df = load_and_add_dummy(save_dir_name, \"Psychology\", config.PYSCHOLOGY)\n",
    "    combined_df = pd.concat([istb_4_df, psychology_north_df, psychology_df], ignore_index=True)\n",
    "\n",
    "    # Fill NaNs in dummy columns with 0\n",
    "    dummy_columns = [ config.ISTB4, config.PYSCHOLOGY, config.PSYCHOLOGY_NORTH]\n",
    "    combined_df[dummy_columns] = combined_df[dummy_columns].fillna(0).astype(int)\n",
    "\n",
    "    # Convert the 'DateTime' column to datetime type\n",
    "    combined_df[config.DATE_TIME] = pd.to_datetime(combined_df[config.DATE_TIME])\n",
    "\n",
    "    # Remove the bldgname column and CHWTON since we are predicting CHWTON/SQM\n",
    "    combined_df.drop([config.BLDGNAME, \"CHWTON\"], axis=1, inplace=True)\n",
    "\n",
    "    # Add columns for the sum of Short and Long wave\n",
    "    # Sum of shortwave and longwave for Top\n",
    "    combined_df['SumW_Top'] = combined_df['ShortW_Top'] + combined_df['LongW_Top']\n",
    "\n",
    "    # Sum of shortwave and longwave for East\n",
    "    combined_df['SumW_East'] = combined_df['ShortW_East'] + combined_df['LongW_East']\n",
    "\n",
    "    # Sum of shortwave and longwave for South\n",
    "    combined_df['SumW_South'] = combined_df['ShortW_South'] + combined_df['LongW_South']\n",
    "\n",
    "    # Sum of shortwave and longwave for West\n",
    "    combined_df['SumW_West'] = combined_df['ShortW_West'] + combined_df['LongW_West']\n",
    "\n",
    "    # Sum of shortwave and longwave for North\n",
    "    combined_df['SumW_North'] = combined_df['ShortW_North'] + combined_df['LongW_North']\n",
    "\n",
    "    # Save as csv\n",
    "    combined_df.to_csv(f\"{save_dir_name}/{save_file_name}.csv\", index=False)\n",
    "\n",
    "    \n",
    "combine_csv(config.BASE_PATH, config.THREE_BLDGS_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d48f90-97bd-4773-b59f-a01e7b5e8566",
   "metadata": {},
   "source": [
    "## 1.2 Save Train and Test Data as CSV (Test will be all data from July 7th). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29d5d79f-cc7e-442c-82ea-314fb56673be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['KW', 'KW/SQM', 'CHWTON/SQM', 'HTmmBTU', 'HTmmBTU/SQM', 'AirT_North',\n",
       "       'AirT_East', 'AirT_South', 'AirT_West', 'AirT_Mean', 'RelH_Mean',\n",
       "       'AbsH_Mean', 'ShortW_North', 'ShortW_East', 'ShortW_South',\n",
       "       'ShortW_West', 'ShortW_Top', 'LongW_North', 'LongW_East', 'LongW_South',\n",
       "       'LongW_West', 'LongW_Top', 'Shade_North', 'Shade_East', 'Shade_South',\n",
       "       'Shade_West', 'Shade_Top', 'Area_North', 'Area_East', 'Area_South',\n",
       "       'Area_West', 'Area_Top', 'SumW_North', 'SumW_East', 'SumW_South',\n",
       "       'SumW_West', 'SumW_Top', 'bldgname_ISTB 4', 'bldgname_Psychology North',\n",
       "       'bldgname_Psychology'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the original csv before broken down to test, train.\n",
    "three_bldgs_df = pd.read_csv(f\"{config.BASE_PATH}/{config.THREE_BLDGS_FILENAME}.csv\")\n",
    "\n",
    "# Set the 'DateTime' column as the index of the DataFrame.\n",
    "three_bldgs_df.set_index(config.DATE_TIME, inplace=True)\n",
    "\n",
    "# Get data for July 7th 2023 and save as test_df. \n",
    "three_bldgs_df.index = pd.to_datetime(three_bldgs_df.index)\n",
    "test_df = three_bldgs_df.loc[config.TEST_DATE]\n",
    "\n",
    "# Get the remaining data for training (all data that's not in test_df).\n",
    "train_df = three_bldgs_df.drop(test_df.index)\n",
    "\n",
    "# Save the test and training sets to new CSV files\n",
    "test_df.to_csv(config.TEST_FILE_PATH)\n",
    "train_df.to_csv(config.TRAIN_FILE_PATH)\n",
    "\n",
    "train_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fd10a-6629-44c3-b201-175f96bdc824",
   "metadata": {},
   "source": [
    "# 2. Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1ce865c-68b1-407e-a28e-0eaddc05bff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(object):\n",
    "    \"\"\"\n",
    "    This class encapsulates the datas that we will need for training, testing, and scoring.\n",
    "    It contains getters for the train and test data.\n",
    "    It also contains a method that creates a df with bldgname column (undummified df) and prediction, actual columns\n",
    "    \"\"\"\n",
    "    def __init__(self, train_file_path, test_file_path, dropped_cols=[]):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            train_file_path (str) : The file path for the trainning csv file. \n",
    "            test_file_path (str) : The path for the test csv file. \n",
    "        \n",
    "        Both train and test datas have columns with buildings already encoded.\n",
    "        \"\"\"\n",
    "        \n",
    "        # 0. Load training data from csv and drop the Y variable for training.\n",
    "        self.train_val_df = pd.read_csv(train_file_path, index_col=0)\n",
    "        dropped_cols.extend([config.CHWTON_SQM])\n",
    "            \n",
    "        # 1. Get X and y train and validation dataframe.\n",
    "        self.X_train_val = self.train_val_df.drop(columns=dropped_cols)\n",
    "        self.y_train_val = self.train_val_df[config.CHWTON_SQM]  \n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.X_train_val, \n",
    "                                                      self.y_train_val, \n",
    "                                                      test_size=0.3, \n",
    "                                                      random_state=20)\n",
    "        \n",
    "        # 2. Get X and y test data frame.\n",
    "        self.test_df = pd.read_csv(test_file_path, index_col=0)\n",
    "        self.X_test = self.test_df.drop(columns=dropped_cols)\n",
    "        self.y_test = self.test_df[config.CHWTON_SQM] \n",
    "        \n",
    "        # 3. Undummify df to get data frame that only has \"index\" and \"bldgname\" which we will use for plotting.\n",
    "        # Drop all the columns apart from 'bldgname' and use date_time as index.\n",
    "        cols_to_keep = [config.BLDGNAME]\n",
    "        \n",
    "        # A. Undummify for Validation\n",
    "        self.df_undum_val = self.undummify(X_val)\n",
    "        self.df_undum_val = self.df_undum_val.drop(columns=self.df_undum_val.columns.difference(cols_to_keep))\n",
    "        \n",
    "        # B. Undummify for Test\n",
    "        self.df_undum_test = self.undummify(self.X_test)\n",
    "        self.df_undum_test = self.df_undum_test.drop(columns=self.df_undum_val.columns.difference(cols_to_keep))\n",
    "\n",
    "        \n",
    "        # 4. Finalise X train and X test data by removing redundant ISTB4 column since it's dummified.\n",
    "#         dropped_cols = [ISTB4]\n",
    "#         self.X_train_val = self.train_val_df.drop(columns=dropped_cols)\n",
    "#         self.X_test = self.test_df.drop(columns=dropped_cols)\n",
    "            \n",
    "        print(\"final X train val=\", self.X_train_val.columns)\n",
    "\n",
    "        \n",
    "    def undummify(self, df, prefix_sep=\"_\"):\n",
    "            \"\"\"\n",
    "            Inner function to undummify pandas df.\n",
    "            \n",
    "            Return:\n",
    "                undummified_df: the undummied df containing only date_time as index and bldgname column\n",
    "            \"\"\"\n",
    "            # 1. drop all columns apart from thos that has bldgname.\n",
    "            # List of all column names\n",
    "            all_columns = df.columns\n",
    "\n",
    "            # List of columns that start with 'bldgname'\n",
    "            bldgname_columns = [col for col in all_columns if col.startswith('bldgname')]\n",
    "\n",
    "            # Drop all columns except those that start with 'bldgname'\n",
    "            df = df[bldgname_columns]\n",
    "            \n",
    "            cols2collapse = {\n",
    "                item.split(prefix_sep)[0]: (prefix_sep in item) for item in df.columns\n",
    "            }\n",
    "            series_list = []\n",
    "            for col, needs_to_collapse in cols2collapse.items():\n",
    "                if needs_to_collapse:\n",
    "                    undummified = (\n",
    "                        df.filter(like=col)\n",
    "                        .idxmax(axis=1)\n",
    "                        .apply(lambda x: x.split(prefix_sep, maxsplit=1)[1])\n",
    "                        .rename(col)\n",
    "                    )\n",
    "                    series_list.append(undummified)\n",
    "                else:\n",
    "                    series_list.append(df[col])\n",
    "            undummified_df = pd.concat(series_list, axis=1)\n",
    "            return undummified_df\n",
    "        \n",
    "    def get_xy_trainval(self):\n",
    "        \"\"\"\n",
    "        Return the X and y for training data which we can split to train and validation data later.\n",
    "        \"\"\"\n",
    "        return self.X_train_val, self.y_train_val\n",
    "    \n",
    "    def get_xy_test(self):\n",
    "        \"\"\"\n",
    "        Return the X and y for test data\n",
    "        \"\"\"\n",
    "        return self.X_test, self.y_test\n",
    "    \n",
    "    def create_prediction_actual_df_input(self, y_pred, y_actual, df):\n",
    "        df_undum = self.undummify(df)\n",
    "\n",
    "        # 1. Add new predict column.\n",
    "        first_plot_name = \"Predicted\"\n",
    "        df_undum[first_plot_name] = y_pred\n",
    "        \n",
    "        # 2\n",
    "        second_plot_name = \"Actual\"\n",
    "        df_undum[second_plot_name] = y_actual\n",
    " \n",
    "\n",
    "        df_bldgs = []\n",
    "        # 4. split df by bldgname\n",
    "        for bldgname in df_undum[config.BLDGNAME].unique():\n",
    "            # - get the df by name\n",
    "            df_bldg = df_undum[df_undum[config.BLDGNAME] == bldgname]\n",
    "\n",
    "            # - only get bldgname, predicted, and actual column.\n",
    "            df_bldg = df_bldg[[config.BLDGNAME, first_plot_name, second_plot_name]]\n",
    "\n",
    "            # - save this df in list\n",
    "            df_bldgs.append(df_bldg)\n",
    "\n",
    "        return [df_undum, df_bldgs]\n",
    "    \n",
    "    def create_prediction_actual_df(self, y_pred, y_actual, is_val=False):\n",
    "        \"\"\"\n",
    "        Create 4 dataframes that consist of \"bldgname, base_prediction, scenario_prediction/actual\" columns. \n",
    "        it will be returned in the following format:\n",
    "        [df_combined, [df_bldg1, df_bldg2, df_bldg3]]\n",
    "        \"\"\"\n",
    "        \n",
    "        # 1. Create undumify df we want to use.\n",
    "        if is_val:\n",
    "            df_undum = self.df_undum_val.copy()\n",
    "        else:\n",
    "            df_undum = self.df_undum_test.copy()\n",
    "\n",
    "\n",
    "        # 2. Add new predict column.\n",
    "        first_plot_name = \"Predicted\"\n",
    "        df_undum[first_plot_name] = y_pred\n",
    "\n",
    "        \n",
    "        # 3. set the second column\n",
    "        second_plot_name = \"Actual\"\n",
    "        df_undum[second_plot_name] = y_actual\n",
    "        \n",
    "        # 4. split df by bldgname\n",
    "        df_bldgs = []\n",
    "        for bldgname in df_undum[config.BLDGNAME].unique():\n",
    "            # - get the df by name\n",
    "            df_bldg = df_undum[df_undum[config.BLDGNAME] == bldgname]\n",
    "            \n",
    "            # - only get bldgname, predicted, and actual column.\n",
    "            df_bldg = df_bldg[[config.BLDGNAME, first_plot_name, second_plot_name]]\n",
    "            \n",
    "            # - save this df in list\n",
    "            df_bldgs.append(df_bldg)\n",
    "            \n",
    "            \n",
    "        return [df_undum, df_bldgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcd0b9-98f3-4638-8038-9fe520d7fa8e",
   "metadata": {},
   "source": [
    "# 3. Train Test Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164d8a7e-fa55-4c2e-9579-b71fc42d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainTest(object):\n",
    "    \"\"\"\n",
    "    This class encapsulates the the training, testing, and plotting process.\n",
    "    It stores the train and test datas that's already split to X and y.\n",
    "    It also stores all the models that has been trained with this dataset.\n",
    "    \"\"\"\n",
    "    def __init__(self, X_train_val, y_train_val, X_test, y_test):\n",
    "        \n",
    "        # - scores_df to display the scores for all our models\n",
    "        self.columns=['model','r2_val', 'r2_test', 'rmse_test','mbe_test']\n",
    "        self.scores_df= pd.DataFrame(columns=self.columns)\n",
    "        \n",
    "        # - train and test data\n",
    "        self.X_train_val = X_train_val\n",
    "        self.y_train_val = y_train_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.model_list = []\n",
    "            \n",
    "    def get_scores_df(self):\n",
    "        return self.scores_df\n",
    "    \n",
    "    \n",
    "    def train_and_store_score(self, model, model_name):\n",
    "        \"\"\"\n",
    "        This function will train the model given as parameter using the training data. It will compute the r2 validation score\n",
    "        and append this as a new row to scores_df.\n",
    "        \n",
    "        Parameters:\n",
    "            model (regressor model) : The model object that will be trained and used in validation.\n",
    "                It can be RF, XGB, LGBM, or catboost regressor\n",
    "                \n",
    "            model_name (str) : the name of the model displayed in scores_df\n",
    "            \n",
    "        Return:\n",
    "            model : This is relevant if we are doing randomized search. it will return the best model\n",
    "        \"\"\"\n",
    "        print(\"\\nmodel_name:\", model_name)\n",
    "        # 1. Train-Val Split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.X_train_val, \n",
    "                                                          self.y_train_val, \n",
    "                                                          test_size=0.3, \n",
    "                                                          random_state=20)\n",
    " \n",
    "        # 2. Fit model and time the training time.\n",
    "        start_time = datetime.datetime.now()\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        end_time = datetime.datetime.now()\n",
    "        \n",
    "        dur_s = (end_time - start_time).total_seconds()\n",
    "\n",
    "        print(\"training duration:\", dur_s)\n",
    "        \n",
    "        # - Get best params if it's a random or grid search\n",
    "        if((\"random\" in model_name)) or (\"grid\" in model_name):\n",
    "            model = model.best_estimator_\n",
    "            \n",
    "        # 3. Get validation R2 score.\n",
    "        val_r2 = model.score(X_val, y_val)\n",
    "        \n",
    "        # 4. Store the training scores.\n",
    "        new_row_data = {'model':model_name, \n",
    "                        \"r2_val\":val_r2, \n",
    "                        \"r2_test\":0, \n",
    "                        'rmse_test':0, \n",
    "                        'mbe_test':0, \n",
    "                        'train_time_s': dur_s}\n",
    "        new_row = pd.DataFrame.from_records([new_row_data])\n",
    "        self.scores_df = pd.concat([self.scores_df, new_row])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_MBE(self, y_true, y_pred):\n",
    "        '''\n",
    "        Parameters:\n",
    "            y_true (array): Array of observed values\n",
    "            y_pred (array): Array of prediction values\n",
    "\n",
    "        Returns:\n",
    "            mbe (float): Bias score\n",
    "        '''\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = y_true.reshape(len(y_true),1)\n",
    "        y_pred = y_pred.reshape(len(y_pred),1)   \n",
    "        diff = (y_pred-y_true)\n",
    "        mbe = diff.mean()\n",
    "        return mbe\n",
    "\n",
    "        \n",
    "    def test_and_store_score(self, model, model_name):\n",
    "        \"\"\"\n",
    "        This function will use the given trained model to compute the y_pred using the X_test data.\n",
    "        It will then compute the mbe, r2, and rmse result and insert it to scores_df. \n",
    "        \n",
    "        Parameters:\n",
    "            model (regressor model): The model that has been trained and will be used to predict y using the test data\n",
    "                It can be RF, XGB, LGBM, or catboost regressor\n",
    "            model_name (string): the name of the model displayed in scores_df\n",
    "        \"\"\"\n",
    "        \n",
    "        # print(model.get_params())\n",
    "        # 1. Get prediction for the test data\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        # 2. Get the three scores\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        mbe = self.get_MBE(self.y_test, y_pred)\n",
    "        \n",
    "        # 3. Update scores_df with the 3 scores above\n",
    "        row_to_update = self.scores_df[\"model\"] == model_name\n",
    "        \n",
    "        print(f\"test_score: r2={r2}, rmse={rmse}, mbe={mbe}\")\n",
    "        \n",
    "        if row_to_update.empty:\n",
    "            # Insert new row with all the values and test scores if it doesn't exist yet.\n",
    "            new_row_data = {\n",
    "                'model':model_name, \n",
    "                \"r2_val\":0, \n",
    "                \"r2_test\":r2, \n",
    "                'rmse_test':rmse, \n",
    "                'mbe_test':mbe, \n",
    "                'train_time_s': 0}\n",
    "            new_row = pd.DataFrame.from_records([new_row_data])\n",
    "            self.scores_df = pd.concat([self.scores_df, new_row])\n",
    "            \n",
    "        else:\n",
    "            # Append just the test score.\n",
    "            col_to_update = ['r2_test','rmse_test', 'mbe_test']\n",
    "            self.scores_df.loc[row_to_update, col_to_update] = [r2, rmse, mbe]\n",
    "        \n",
    "        \n",
    "    def train_test_models(self, model_list):\n",
    "        \"\"\"\n",
    "        This function will take a list of models and train them. if the model is from randomized search, it will\n",
    "        return the model with best params. This best model will be used to test and store score.\n",
    "        \n",
    "        Parameters:\n",
    "            model_list (regressor models): list of all models that will be trained, tested, and plot the SHAP values\n",
    "                It can be RF, XGB, LGBM, or catboost regressor        \n",
    "        \"\"\"\n",
    "        \n",
    "        for model_name, model in model_list:\n",
    "            # - We need to do this in case its randomized search.\n",
    "            best_model = self.train_and_store_score(model, model_name)\n",
    "            \n",
    "            # - Assign the model to model_list.\n",
    "            self.model_list.append((best_model, model_name))\n",
    "            \n",
    "            self.test_and_store_score(best_model, model_name)\n",
    "            \n",
    "    \n",
    "    def get_all_models(self):\n",
    "        return self.model_list\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de70af-4c6b-4832-95c6-bbab58a61c97",
   "metadata": {},
   "source": [
    "# 5. Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc8cd028-05e1-499a-9f4b-60fb46e6701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, name, base_model, random_grid, cv, n_iter):\n",
    "        # - base model\n",
    "        self.base_name = name + \"_base\"\n",
    "        self.base = base_model\n",
    "        \n",
    "        # - randomized search model\n",
    "        self.random_name = name + \"_random\"\n",
    "        self.random = RandomizedSearchCV(\n",
    "                            estimator = self.base,\n",
    "                            param_distributions = random_grid,\n",
    "                            n_iter = n_iter,\n",
    "                            cv = cv,\n",
    "                            verbose = 0,\n",
    "                            random_state = config.RANDOM_STATE,\n",
    "                            scoring ='r2',\n",
    "                            n_jobs = -1)\n",
    "        \n",
    "        # - grid search model\n",
    "        self.grid_name = name + \"_grid\"\n",
    "        self.reg_grid = None\n",
    "        \n",
    "        \n",
    "    def set_get_reg_grid(self, grid_param):\n",
    "        self.reg_grid = GridSearchCV(\n",
    "                        estimator =self.base,\n",
    "                        param_grid = grid_param,\n",
    "                        cv = self.cv,\n",
    "                        scoring ='r2',\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "        return self.reg_grid\n",
    "    \n",
    "    \n",
    "    # model getters\n",
    "    def get_base_model(self):\n",
    "        return self.base\n",
    "\n",
    "    def get_random_model(self):\n",
    "        return self.random\n",
    "    \n",
    "    def get_grid_model(self):\n",
    "        return self.grid\n",
    "    \n",
    "    # name getters\n",
    "    def get_base_name(self):\n",
    "        return self.base_name\n",
    "    \n",
    "    def get_random_name(self):\n",
    "        return self.random_name\n",
    "\n",
    "    \n",
    "    def get_grid_name(self):\n",
    "        return self.grid_name\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc48920-085b-47f8-8dcb-1c284843bc42",
   "metadata": {},
   "source": [
    "# 7. Train, Test, and Save Scores for All Models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612b179-6618-440e-8e0d-c0c35db731c5",
   "metadata": {},
   "source": [
    "## Note:\n",
    "1. Drop istb_4 column since its redundant for all models apart from RF.\n",
    "2. For each of the models, we have base model and randomized tuned model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89ef49-b928-4dcb-9421-eb907c578db7",
   "metadata": {},
   "source": [
    "## 7.1. Train, Test, and Save Scores for Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3aa8df3-c253-4aff-829e-87780a7ac429",
   "metadata": {},
   "source": [
    "### 7.1.0 Prepare the Combinations of Columns to Drop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8298c97f-b56c-4dc3-8d28-bdbcb5351c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional variables on top of those from first publication.\n",
    "EXTRA_VARIABLES=[\n",
    "    'KW/SQM', \n",
    "    'HTmmBTU/SQM',\n",
    "    'RelH_Mean',   \n",
    "    'ShortW_Top', \n",
    "    'Shade_Top', \n",
    "    'AirT_North', 'AirT_East', 'AirT_South', 'AirT_West',\n",
    "    'LongW_North', 'LongW_East', 'LongW_South', 'LongW_West', 'LongW_Top', \n",
    "    'SumW_North', 'SumW_East', 'SumW_South', 'SumW_West', 'SumW_Top',\n",
    "    'Area_North', 'Area_East', 'Area_South','Area_West', 'Area_Top', \n",
    "]\n",
    "\n",
    "\n",
    "# Declare lists of columns that we want to drop for each train-test execution and \n",
    "# the title to summarize the columns we are dropping.\n",
    "# Rel Humidity\n",
    "REL_HUMID_COLUMN = ['RelH_Mean']\n",
    "\n",
    "# KW and HTTMBTU\n",
    "KW_SQM_COLUMN = ['KW/SQM']\n",
    "HTMMBTU_SQM_COLUMN = ['HTmmBTU/SQM']\n",
    "KW_COLUMN = ['KW']\n",
    "HTMMBTU_COLUMN = ['HTmmBTU']\n",
    "\n",
    "# Top\n",
    "SHORT_WAVE_TOP_COLUMN = ['ShortW_Top']\n",
    "SHADE_TOP_COLUMN = ['Shade_Top']\n",
    "\n",
    "# AirTemp\n",
    "AIR_TEMP_FACADE_COLUMNS = ['AirT_North', 'AirT_East', 'AirT_South', 'AirT_West']\n",
    "AREA_COLUMNS = ['Area_North', 'Area_East', 'Area_South', 'Area_West', 'Area_Top']\n",
    "\n",
    "# Long and Short wave\n",
    "LONG_WAVE_COLUMNS = ['LongW_North', 'LongW_East', 'LongW_South', 'LongW_West', 'LongW_Top']\n",
    "SHORT_WAVE_COLUMNS = ['ShortW_North','ShortW_East', 'ShortW_South', 'ShortW_West']\n",
    "SUM_WAVE_COLUMNS = ['SumW_North', 'SumW_East', 'SumW_South', 'SumW_West', 'SumW_Top']\n",
    "\n",
    "# We also want to test not using short wave and just use sum of short and long wave instead.\n",
    "EXTRA_AND_SHORT_WAVE = EXTRA_VARIABLES + SHORT_WAVE_COLUMNS\n",
    "\n",
    "# To include.\n",
    "sumWave_area_airtempFacade_shortWaveTop_shadeTop = SUM_WAVE_COLUMNS + AREA_COLUMNS + AIR_TEMP_FACADE_COLUMNS + SHORT_WAVE_TOP_COLUMN + SHADE_TOP_COLUMN\n",
    "\n",
    "\n",
    "dropped_title_columns_list = [\n",
    "\n",
    "    # OPTION1: Use SQM for HTMMBTU and KW.    \n",
    "    # (\"area_dropped\", AREA_COLUMNS),\n",
    "    \n",
    "    # (\"airtemp_facade_dropped\", AIR_TEMP_FACADE_COLUMNS + AREA_COLUMNS),\n",
    "\n",
    "    (\"ali_proposal\", AIR_TEMP_FACADE_COLUMNS + AREA_COLUMNS + SUM_WAVE_COLUMNS + LONG_WAVE_COLUMNS + HTMMBTU_COLUMN + HTMMBTU_SQM_COLUMN + KW_COLUMN + REL_HUMID_COLUMN ),\n",
    "    \n",
    "    # Use sumwave columns.\n",
    "    # (\"area_airtemp_facade_dropped_3\", AIR_TEMP_FACADE_COLUMNS + AREA_COLUMNS + HTMMBTU_COLUMN + KW_COLUMN ),\n",
    "\n",
    "    # OPTION2: Use NON-SQM for HTMMBTU and KW (As in the previous publication).\n",
    "    # 3. Use non sqm for htmmbtu and kw\n",
    "    # (\"area_airtemp_facade_dropped_4\", AIR_TEMP_FACADE_COLUMNS + AREA_COLUMNS + HTMMBTU_SQM_COLUMN + KW_SQM_COLUMN),\n",
    "\n",
    "    # 4. Drop all columns in extra variables except for relative humidity.\n",
    "    # (\"match_previous_data_incl_rel_humid\", [col for col in EXTRA_VARIABLES if col !=REL_HUMID_COLUMN[0]]),\n",
    "    \n",
    "    # 5. Drop all columns in extra variables except for the sumwave columns.\n",
    "    # (\"match_previous_data_incl_sumwave\", [col for col in EXTRA_VARIABLES if col not in SUM_WAVE_COLUMNS]),\n",
    "    \n",
    "    # 6. Drop all columns in extra variables except for sumwave, area, airtemp facade, shortWave top and shade top. \n",
    "    # (\"match_previous_data_incl_sumwave_area_airtempFacade_shortWTop_shadeTop\", [col for col in EXTRA_VARIABLES if col not in sumWave_area_airtempFacade_shortWaveTop_shadeTop]),\n",
    "\n",
    "    # 7. Drop all columns in extra variables.\n",
    "    # (\"match_previous_data\", EXTRA_VARIABLES ), \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f97e45e7-413a-4361-aead-6730ea950496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dropped title:  ali_proposal\n",
      "final X train val= Index(['KW/SQM', 'AirT_Mean', 'AbsH_Mean', 'ShortW_North', 'ShortW_East',\n",
      "       'ShortW_South', 'ShortW_West', 'ShortW_Top', 'Shade_North',\n",
      "       'Shade_East', 'Shade_South', 'Shade_West', 'Shade_Top',\n",
      "       'bldgname_ISTB 4', 'bldgname_Psychology North', 'bldgname_Psychology'],\n",
      "      dtype='object')\n",
      "                       KW/SQM  AirT_Mean  AbsH_Mean  ShortW_North  \\\n",
      "Date_Time                                                           \n",
      "2023-05-03 05:00:00  0.041512  22.848720   4.987361           0.0   \n",
      "2023-05-03 05:15:00  0.042342  22.670915   5.452283           0.0   \n",
      "2023-05-03 05:30:00  0.041332  22.377459   5.329339           0.0   \n",
      "2023-05-03 05:45:00  0.041755  22.221263   5.312129           0.0   \n",
      "2023-05-03 06:00:00  0.041877  22.055966   5.288323           0.0   \n",
      "...                       ...        ...        ...           ...   \n",
      "2023-07-18 13:30:00  0.025110  43.867376   9.304210           0.0   \n",
      "2023-07-18 13:45:00  0.023868  44.059024   9.281645           0.0   \n",
      "2023-07-18 14:00:00  0.023449  44.256798   9.248373           0.0   \n",
      "2023-07-18 14:15:00  0.023388  44.415822   9.196790           0.0   \n",
      "2023-07-18 14:30:00  0.023878  44.556228   9.114345           0.0   \n",
      "\n",
      "                     ShortW_East  ShortW_South  ShortW_West   ShortW_Top  \\\n",
      "Date_Time                                                                  \n",
      "2023-05-03 05:00:00          0.0      0.000000     0.000000     0.000000   \n",
      "2023-05-03 05:15:00          0.0      0.000000     0.000000     0.000000   \n",
      "2023-05-03 05:30:00          0.0      0.000000     0.000000     0.000000   \n",
      "2023-05-03 05:45:00          0.0      0.000000     0.000000     0.000000   \n",
      "2023-05-03 06:00:00          0.0      0.000000     0.000000     0.000000   \n",
      "...                          ...           ...          ...          ...   \n",
      "2023-07-18 13:30:00          0.0     72.095923    80.463530  1054.249724   \n",
      "2023-07-18 13:45:00          0.0     68.913908   100.354759  1033.664338   \n",
      "2023-07-18 14:00:00          0.0     59.100402   145.444498   983.170678   \n",
      "2023-07-18 14:15:00          0.0     54.632761   172.665707   952.210446   \n",
      "2023-07-18 14:30:00          0.0     45.027442   223.542406   885.203320   \n",
      "\n",
      "                     Shade_North  Shade_East  Shade_South  Shade_West  \\\n",
      "Date_Time                                                               \n",
      "2023-05-03 05:00:00          1.0         1.0     1.000000    1.000000   \n",
      "2023-05-03 05:15:00          1.0         1.0     1.000000    1.000000   \n",
      "2023-05-03 05:30:00          1.0         1.0     1.000000    1.000000   \n",
      "2023-05-03 05:45:00          1.0         1.0     1.000000    1.000000   \n",
      "2023-05-03 06:00:00          1.0         1.0     1.000000    1.000000   \n",
      "...                          ...         ...          ...         ...   \n",
      "2023-07-18 13:30:00          1.0         1.0     0.098086    0.279221   \n",
      "2023-07-18 13:45:00          1.0         1.0     0.093301    0.344156   \n",
      "2023-07-18 14:00:00          1.0         1.0     0.098086    0.370130   \n",
      "2023-07-18 14:15:00          1.0         1.0     0.098086    0.334416   \n",
      "2023-07-18 14:30:00          1.0         1.0     0.088517    0.318182   \n",
      "\n",
      "                     Shade_Top  bldgname_ISTB 4  bldgname_Psychology North  \\\n",
      "Date_Time                                                                    \n",
      "2023-05-03 05:00:00     1.0000                1                          0   \n",
      "2023-05-03 05:15:00     1.0000                1                          0   \n",
      "2023-05-03 05:30:00     1.0000                1                          0   \n",
      "2023-05-03 05:45:00     1.0000                1                          0   \n",
      "2023-05-03 06:00:00     1.0000                1                          0   \n",
      "...                        ...              ...                        ...   \n",
      "2023-07-18 13:30:00     0.0080                0                          0   \n",
      "2023-07-18 13:45:00     0.0144                0                          0   \n",
      "2023-07-18 14:00:00     0.0224                0                          0   \n",
      "2023-07-18 14:15:00     0.0224                0                          0   \n",
      "2023-07-18 14:30:00     0.0224                0                          0   \n",
      "\n",
      "                     bldgname_Psychology  \n",
      "Date_Time                                 \n",
      "2023-05-03 05:00:00                    0  \n",
      "2023-05-03 05:15:00                    0  \n",
      "2023-05-03 05:30:00                    0  \n",
      "2023-05-03 05:45:00                    0  \n",
      "2023-05-03 06:00:00                    0  \n",
      "...                                  ...  \n",
      "2023-07-18 13:30:00                    1  \n",
      "2023-07-18 13:45:00                    1  \n",
      "2023-07-18 14:00:00                    1  \n",
      "2023-07-18 14:15:00                    1  \n",
      "2023-07-18 14:30:00                    1  \n",
      "\n",
      "[4523 rows x 16 columns]\n",
      "\n",
      "model_name: catboost_base\n",
      "training duration: 1.928932\n",
      "test_score: r2=0.9556067765829956, rmse=0.0011251373159982274, mbe=0.000145613553604216\n",
      "\n",
      "model_name: catboost_random\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training duration: 78.863169\n",
      "test_score: r2=0.9509618951809503, rmse=0.0011825350641268, mbe=0.0001800143597989586\n"
     ]
    }
   ],
   "source": [
    "# Intialize all models with base and randomized search version.\n",
    "RF = Model(config.rf_name, config.rf_base, config.rf_param, config.CV, config.N_ITER)\n",
    "XGB = Model(config.xgb_name, config.xgb_base, config.xgb_param, config.CV,config. N_ITER)\n",
    "LGBM = Model(config.lgbm_name, config.lgbm_base, config.lgbm_param, config.CV, config.N_ITER)\n",
    "CB = Model(config.cb_name, config.cb_base, config.cb_param, config.CV, config.N_ITER)\n",
    "# model_objects = [RF, XGB, LGBM, CB]\n",
    "model_objects = [CB]\n",
    "\n",
    "# Iterate through each of the dropped columns list and execute train, test, and save scores to csv.\n",
    "for dropped_title, dropped_cols in dropped_title_columns_list:\n",
    "    print(\"\\nDropped title: \", dropped_title)\n",
    "    \n",
    "    # 0. Initialize data objects.\n",
    "    data_obj = Data(config.TRAIN_FILE_PATH, config.TEST_FILE_PATH, dropped_cols)\n",
    "        \n",
    "    # 1. Get the validation and test data.\n",
    "    X_train_val, y_train_val = data_obj.get_xy_trainval()\n",
    "    X_test, y_test = data_obj.get_xy_test()\n",
    "    print(X_train_val)\n",
    "    \n",
    "    # 2. Init the trainTest object with the loaded datas.\n",
    "    tt = TrainTest(X_train_val, y_train_val, X_test, y_test)\n",
    "    \n",
    "    # 3. Get all models.\n",
    "    all_base_random = []\n",
    "    for model in model_objects:\n",
    "        # Get the base and randomized search version of each model.\n",
    "        base_name, base_model = model.get_base_name(), model.get_base_model()\n",
    "        random_name, random_model = model.get_random_name(), model.get_random_model()\n",
    "        all_base_random.extend([(base_name, base_model), (random_name, random_model)])\n",
    "\n",
    "\n",
    "    # 4. Train and test.\n",
    "    tt.train_test_models(all_base_random)\n",
    "    \n",
    "    # 5. Save results. \n",
    "    scores_df = tt.get_scores_df()\n",
    "    scores_df.to_csv(f\"{config.RESULT_DIR_PATH}/scores_{dropped_title}.csv\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1de1b97-7644-4ed0-9f81-b205aaa95efb",
   "metadata": {},
   "source": [
    "# Test getting important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e5a08-0f8e-4556-9c57-f1bed1b28346",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. init model object\n",
    "# RF = Model(rf_name, rf_base, rf_random_grid, CV, N_ITER)\n",
    "# XGB = Model(xgb_name, xgb_base, xgb_random_grid, CV, N_ITER)\n",
    "# LGBM = Model(lgbm_name, lgbm_base, lgbm_random_grid, CV, N_ITER)\n",
    "# # CB = Model(catboost_name, catboost_base, catboost_random_grid, CV, N_ITER)\n",
    "\n",
    "# all_base_random = []\n",
    "\n",
    "# # 5. add the rest\n",
    "# # model_objects = [XGB, LGBM, CB]\n",
    "# model_objects = [RF, XGB, LGBM]\n",
    "# for model in model_objects:\n",
    "#     base_name, base_model = model.get_base_name(), model.get_base_model()\n",
    "#     random_name, random_model = model.get_random_name(), model.get_random_model()\n",
    "#     all_base_random.extend([(base_name, base_model), (random_name, random_model)])\n",
    "\n",
    "\n",
    "# # 6. train, test all models\n",
    "# tt.train_test_models(all_base_random)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d2f5fc-3f2d-4d50-8ae6-49b7f10d03b6",
   "metadata": {},
   "source": [
    "# 9. Save all Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1b3ce2-7c59-4c9e-934c-b51864a31f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models = tt.get_all_models()\n",
    "# set file \n",
    "lgbm = None\n",
    "for model, name in all_models:\n",
    "    print(\"name=\", name)\n",
    "    # 1. Print the actual scores.\n",
    "    tt.test_and_store_score(model, name)\n",
    "\n",
    "    # 2. Create the path to save the models.\n",
    "    save_path = config.MODEL_DIR_PATH\n",
    "    isExist = os.path.exists(save_path)\n",
    "\n",
    "    if not isExist:\n",
    "       # Create a new directory if it does not exist\n",
    "       os.makedirs(save_path)\n",
    "\n",
    "    # 3. Save the models.\n",
    "    if \"lgbm\" in name:\n",
    "        print(\"lgbm>>\")\n",
    "        # Set the filename.\n",
    "        filename = name + \".pkl\"\n",
    "\n",
    "        #  Save to the model to filepath.\n",
    "        joblib.dump(model, f\"{save_path}/{filename}\")\n",
    "\n",
    "\n",
    "        # Reload and check if the scores matches.\n",
    "        model = joblib.load(save_path + filename)\n",
    "        tt.test_and_store_score(model, name)\n",
    "\n",
    "    else:\n",
    "        # Set the filename.\n",
    "        filename = name + \".sav\"\n",
    "\n",
    "        # Save to the model to filepath\n",
    "        pickle.dump(model, open(f\"{save_path}/{filename}\", 'wb'))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e566dc-8762-46f5-915b-cab2ebfa245a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
