{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba628875-88b7-4b70-bbaa-4665018d58af",
   "metadata": {},
   "source": [
    "Use the final csv files for train and test and use various models to train and score them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3470fa-5ba9-4de4-8bbe-c6562636c324",
   "metadata": {},
   "source": [
    "# 1. Import and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8eec3d-cd80-40fd-8f52-fc1c339babbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# uncomment to install the three models below >>>>>\n",
    "# !pip3 install catboost\n",
    "# !pip install lightgbm\n",
    "# !pip3 install xgboost\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "\n",
    "# parameters search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# scoring\n",
    "import math\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# explain\n",
    "import shap\n",
    "import datetime\n",
    "\n",
    "# save model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972dc900-b2a2-4bbf-8380-36c7b7838610",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../Data/microclimate_model/Combined/three_bldgs_dropped.csv\"\n",
    "TEST_PATH = \"../Data/microclimate_model/Combined/three_bldgs_J9_dropped.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fd10a-6629-44c3-b201-175f96bdc824",
   "metadata": {},
   "source": [
    "# 2. Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddd56cb-10f9-42c0-ab23-7cdc737bdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This class encapsulates the datas that we will need for training and testing.\n",
    "It only contains getters for the train and test data\n",
    "\"\"\"\n",
    "class Data(object):\n",
    "    def __init__(self, train_path, test_path, isDropISTB = True):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            train_path (str) : The file path for the trainning csv file. \n",
    "            test_path (str) : The path for the test csv file. \n",
    "        \n",
    "        Both train and test datas have 16 columns with buildings already encoded\n",
    "        \"\"\"\n",
    "        dropped_cols = ['CHWTON/SQM', \"bldgname_ISTB 4\"]\n",
    "        if not isDropISTB:\n",
    "            dropped_cols = ['CHWTON/SQM']\n",
    "            \n",
    "        # - Train and validation data\n",
    "        self.train_val_df = pd.read_csv(train_path, index_col=0)\n",
    "        self.X_train_val = self.train_val_df.drop(columns=dropped_cols)\n",
    "        self.y_train_val = self.train_val_df['CHWTON/SQM']  \n",
    "        \n",
    "        # - Test data\n",
    "        self.test_df = pd.read_csv(test_path, index_col=0)\n",
    "        self.X_test = self.test_df.drop(columns=dropped_cols)\n",
    "        self.y_test = self.test_df['CHWTON/SQM'] \n",
    "        \n",
    "    \n",
    "    def get_xy_trainval(self):\n",
    "        \"\"\"\n",
    "        Return the X and y for training data which we can split to train and validation data later.\n",
    "        \"\"\"\n",
    "        return self.X_train_val, self.y_train_val\n",
    "    \n",
    "    def get_xy_test(self):\n",
    "        \"\"\"\n",
    "        Return the X and y for June 9th test data\n",
    "        \"\"\"\n",
    "        return self.X_test, self.y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba8d05b-3aa9-4c91-b860-0f9bee6ebd30",
   "metadata": {},
   "source": [
    "# 3. Explainer Class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcd0b9-98f3-4638-8038-9fe520d7fa8e",
   "metadata": {},
   "source": [
    "# 4. Train Test Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "164d8a7e-fa55-4c2e-9579-b71fc42d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class encapsulates the the training, testing, and plotting process.\n",
    "It stores the train and test datas that's already split to X and y\n",
    "\"\"\"\n",
    "class TrainTest(object):\n",
    "    def __init__(self, X_train_val, y_train_val, X_test, y_test):\n",
    "        \n",
    "        # - scores_df to display the scores for all our models\n",
    "        self.columns=['model','r2_val', 'r2_test', 'rmse_test','mbe_test']\n",
    "        self.scores_df= pd.DataFrame(columns=self.columns)\n",
    "        \n",
    "        # - train and test data\n",
    "        self.X_train_val = X_train_val\n",
    "        self.y_train_val = y_train_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        self.tuned_model_list = []\n",
    "            \n",
    "    def get_scores_df(self):\n",
    "        return self.scores_df\n",
    "    \n",
    "    \n",
    "    def train_and_store_score(self, model, model_name):\n",
    "        \"\"\"\n",
    "        This function will train the model given as parameter using the training data. It will compute the r2 validation score\n",
    "        and append this as a new row to scores_df.\n",
    "        \n",
    "        Parameters:\n",
    "            model (regressor model) : The model object that will be trained and used in validation.\n",
    "                It can be RF, XGB, LGBM, or catboost regressor\n",
    "                \n",
    "            model_name (str) : the name of the model displayed in scores_df\n",
    "            \n",
    "        RetursnL\n",
    "            model : this is relevant if we are doing randomized search. it will return the best model\n",
    "        \"\"\"\n",
    "        print(\"\\nmodel_name>>>\", model_name)\n",
    "        # 1. Train-Val Split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.X_train_val, \n",
    "                                                          self.y_train_val, \n",
    "                                                          test_size=0.3, \n",
    "                                                          random_state=20)\n",
    "\n",
    "        # 2. fit model that and time it\n",
    "        start_time = datetime.datetime.now()\n",
    "        print(\"start time>>>\", start_time)\n",
    "        model.fit(X_train, y_train)\n",
    "        end_time = datetime.datetime.now()\n",
    "        print(\"end time>>>>\", end_time)\n",
    "        dur_s = (end_time - start_time).total_seconds()\n",
    "        print(\"dur\", dur_s)\n",
    "        \n",
    "        \n",
    "        # - Get best params if it's a random or grid search\n",
    "        if(\"random\" in model_name) or (\"grid\" in model_name):\n",
    "            model = model.best_estimator_\n",
    "            print(\"params\", model.get_params())\n",
    "            \n",
    "        # 3. get validation R2 score\n",
    "        val_r2 = model.score(X_val, y_val)\n",
    "        \n",
    "        # 4. store score\n",
    "        new_row_data = {'model':model_name, \n",
    "                        \"r2_val\":val_r2, \n",
    "                        \"r2_test\":0, \n",
    "                        'rmse_test':0, \n",
    "                        'mbe_test':0, \n",
    "                        'train_time_s': dur_s}\n",
    "        new_row = pd.DataFrame.from_records([new_row_data])\n",
    "        self.scores_df = pd.concat([self.scores_df, new_row])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_MBE(self, y_true, y_pred):\n",
    "        '''\n",
    "        Parameters:\n",
    "            y_true (array): Array of observed values\n",
    "            y_pred (array): Array of prediction values\n",
    "\n",
    "        Returns:\n",
    "            mbe (float): Bias score\n",
    "        '''\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = y_true.reshape(len(y_true),1)\n",
    "        y_pred = y_pred.reshape(len(y_pred),1)   \n",
    "        diff = (y_pred-y_true)\n",
    "        mbe = diff.mean()\n",
    "        return mbe\n",
    "\n",
    "        \n",
    "    def test_and_store_score(self, model, model_name):\n",
    "        \"\"\"\n",
    "        This function will use the given trained model to compute the y_pred using the X_test data.\n",
    "        It will then compute the mbe, r2, and rmse result and insert it to scores_df. \n",
    "        \n",
    "        Parameters:\n",
    "            model (regressor model): The model that has been trained and will be used to predict y using the test data\n",
    "                It can be RF, XGB, LGBM, or catboost regressor\n",
    "            model_name (string): the name of the model displayed in scores_df\n",
    "        \"\"\"\n",
    "        print(\"\\ntest and store scores..\", model_name)\n",
    "        print(model.get_params())\n",
    "        # 1. Get prediction for the test data\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        # 2. get the three scores\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        mbe = self.get_MBE(self.y_test, y_pred)\n",
    "        \n",
    "        # 3. update scores_df with the 3 scores above\n",
    "        row_to_update = self.scores_df[\"model\"] == model_name\n",
    "        col_to_update = ['r2_test','rmse_test', 'mbe_test']\n",
    "        self.scores_df.loc[row_to_update, col_to_update] = [r2, rmse, mbe]\n",
    "        \n",
    "        \n",
    "    def train_test_models(self, model_list):\n",
    "        \"\"\"\n",
    "        This function will take a list of models and train them. if the model is from randomized search, it will\n",
    "        return the model with best params. This best model will be used to test and store score.\n",
    "        Using the same model, it will plot the SHAP values.\n",
    "        \n",
    "        Parameters:\n",
    "            model_list (regressor models): list of all models that will be trained, tested, and plot the SHAP values\n",
    "                It can be RF, XGB, LGBM, or catboost regressor        \n",
    "        \"\"\"\n",
    "        \n",
    "        for model_name, model in model_list:\n",
    "            # - need to do this in case its randomized search\n",
    "            best_model = self.train_and_store_score(model, model_name)\n",
    "            \n",
    "            # assign the model_list so we can get the best 2\n",
    "            self.tuned_model_list.append((best_model, model_name))\n",
    "            \n",
    "            self.test_and_store_score(best_model, model_name)\n",
    "            \n",
    "            \n",
    "    def get_bestN_random_models(self, n):\n",
    "        \"\"\"\n",
    "        This function will take the best four of r2_test score, grab the name of the models, \n",
    "        and return the actual regressor object\n",
    "        \n",
    "        Parameters:\n",
    "            model_list (regressor models): list of two models and its name as tuple from randomized search        \n",
    "        \"\"\"\n",
    "        # 0. convert columns to numeric\n",
    "        col_to_numeric = [\"r2_val\", \"r2_test\", \"rmse_test\", \"mbe_test\"]\n",
    "        self.scores_df[col_to_numeric] = self.scores_df[col_to_numeric].apply(pd.to_numeric)\n",
    "        \n",
    "        # 1. get best 4 r2\n",
    "        score_best4 = scores_df.nlargest(4, \"r2_test\")\n",
    "        \n",
    "        # 2. get names best2 of randoms\n",
    "        best_random_model_names = []\n",
    "        for row_i in range(n):\n",
    "            \n",
    "            # 3. within the best 4 models, get the best two of random\n",
    "            if score_best4.iloc[row_i, 0][-6:] == \"random\":\n",
    "                best_random_model_names.append(score_best4.iloc[row_i, 0])\n",
    "        \n",
    "        print(\"best_random_model_names\", best_random_model_names)\n",
    "        \n",
    "        # 4. get the model\n",
    "        best_random_models = []\n",
    "        \n",
    "        for tuned_model, model_name in self.tuned_model_list:\n",
    "            \n",
    "            if model_name in best_random_model_names:\n",
    "                print(\"model_name\", model_name)\n",
    "                best_random_models.append((tuned_model, model_name))\n",
    "                \n",
    "        return best_random_models\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de70af-4c6b-4832-95c6-bbab58a61c97",
   "metadata": {},
   "source": [
    "# 5. Model Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc8cd028-05e1-499a-9f4b-60fb46e6701f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    def __init__(self, name, base_model, random_grid):\n",
    "                \n",
    "        # - base model\n",
    "        self.base_name = name + \"_base\"\n",
    "        self.base = base_model\n",
    "        \n",
    "        # - randomized search model\n",
    "        self.random_name = name + \"_random\"\n",
    "        \n",
    "        self.cv = RepeatedKFold(n_splits=10, n_repeats=3)\n",
    "        self.random = RandomizedSearchCV(\n",
    "                            estimator = self.base,\n",
    "                            param_distributions = random_grid,\n",
    "                            n_iter = 20,\n",
    "                            cv = self.cv,\n",
    "                            verbose = 0,\n",
    "                            random_state = 42,\n",
    "                            scoring ='r2',\n",
    "                            n_jobs = -1)\n",
    "        \n",
    "        # - grid search model\n",
    "        self.grid_name = name + \"_grid\"\n",
    "        self.reg_grid = None\n",
    "        \n",
    "        \n",
    "        \n",
    "    def set_get_reg_grid(self, grid_param):\n",
    "        self.reg_grid = GridSearchCV(\n",
    "                        estimator =self.base,\n",
    "                        param_grid = grid_param,\n",
    "                        cv = self.cv,\n",
    "                        scoring ='r2',\n",
    "                        n_jobs = -1)\n",
    "        \n",
    "        return self.reg_grid\n",
    "    \n",
    "    \n",
    "    # model getters\n",
    "    def get_base_model(self):\n",
    "        return self.base\n",
    "    \n",
    "    \n",
    "    def get_random_model(self):\n",
    "        return self.random\n",
    "    \n",
    "    def get_grid_model(self):\n",
    "        return self.grid\n",
    "    \n",
    "    \n",
    "    # name getters\n",
    "    def get_base_name(self):\n",
    "        return self.base_name\n",
    "    \n",
    "    def get_random_name(self):\n",
    "        return self.random_name\n",
    "\n",
    "    \n",
    "    def get_grid_name(self):\n",
    "        return self.grid_name\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b02bfe-eb66-4b46-b087-14a667edd256",
   "metadata": {},
   "source": [
    "# 6. Config for all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3db71091-39a5-478e-a6b6-c82bb99f006a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RF\n",
    "# A. base model\n",
    "rf_name = \"rf\"\n",
    "rf_base = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "\n",
    "# B. grid for random model\n",
    "rf_random_grid = {\n",
    "                # [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "                'n_estimators': [int(x) for x in np.linspace(start = 50, stop = 500, num = 10)],\n",
    "                'max_features': [\"sqrt\", \"log2\", None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "                'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
    "                'criterion': ['squared_error', 'absolute_error', 'poisson'],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [ 1, 2, 4],\n",
    "                'bootstrap': [True] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40bfb61a-b7ff-42b7-902f-d99d978388ab",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XGB\n",
    "xgb_name = \"xgb\"\n",
    "xgb_base = XGBRegressor(n_estimators = 100, verbosity = 0, random_state = 42)\n",
    "\n",
    "xgb_random_grid = {\n",
    "    'learning_rate' : [0.1, 0,2 ,0.3, 0.4],\n",
    "    'n_estimators':[ 100, 250, 500, 1000],\n",
    "    'min_child_weight':[1, 2, 4, 5, 8], \n",
    "    'max_depth': [4,6,7,8],\n",
    "    'colsample_bytree' : [ 0.3, 0.4, 0.5 , 0.7, 1 ],\n",
    "    'booster': ['gbtree', 'gblinear'] }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d6b24a9-0f3e-4ee8-b9e2-b862c9d4a0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LGBM\n",
    "# A. base model\n",
    "lgbm_name = \"lgbm\"\n",
    "lgbm_base = LGBMRegressor(random_state = 42)\n",
    "\n",
    "# B. grid for random model\n",
    "lgbm_random_grid = {'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "                'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000],\n",
    "                'num_leaves': [10,15,20,30,40], \n",
    "                'min_child_samples': [10,20,40,50,100],\n",
    "                'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],\n",
    "                'subsample': [0.1, 0.3, 0.8, 1.0], \n",
    "                'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n",
    "                'colsample_bytree': [0.4, 0.5, 0.6, 1.0],\n",
    "                'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "                'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "936baf26-32ae-468e-8ef6-dcd647fc2627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CATBOOST\n",
    "# A. base model\n",
    "catboost_name = \"catboost\"\n",
    "catboost_base = CatBoostRegressor(random_state = 42, verbose=False)\n",
    "\n",
    "\n",
    "# B. Randomized tuned model \n",
    "catboost_random_grid = {\n",
    "        'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "        'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000],\n",
    "        'depth': [4, 6, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7, 9]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc48920-085b-47f8-8dcb-1c284843bc42",
   "metadata": {},
   "source": [
    "# 7. Run models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d612b179-6618-440e-8e0d-c0c35db731c5",
   "metadata": {},
   "source": [
    "## Note:\n",
    "1. drop istb_4 column since its redundant. increased the RF test result.\n",
    "2. changed the randomized search cv for RF. now the result is quite good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e45e7-413a-4361-aead-6730ea950496",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model_name>>> rf_base\n",
      "start time>>> 2022-11-07 16:20:30.402387\n",
      "end time>>>> 2022-11-07 16:20:30.826656\n",
      "dur 0.424269\n",
      "\n",
      "test and store scores.. rf_base\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': None, 'max_features': 1.0, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "model_name>>> rf_random\n",
      "start time>>> 2022-11-07 16:20:30.843835\n",
      "end time>>>> 2022-11-07 16:24:37.294323\n",
      "dur 246.450488\n",
      "params {'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 100, 'max_features': 0.6, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 250, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "test and store scores.. rf_random\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'criterion': 'squared_error', 'max_depth': 100, 'max_features': 0.6, 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 2, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 250, 'n_jobs': None, 'oob_score': False, 'random_state': 42, 'verbose': 0, 'warm_start': False}\n",
      "\n",
      "model_name>>> xgb_base\n",
      "start time>>> 2022-11-07 16:24:37.329601\n",
      "end time>>>> 2022-11-07 16:24:37.528624\n",
      "dur 0.199023\n",
      "\n",
      "test and store scores.. xgb_base\n",
      "{'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': 0, 'gpu_id': -1, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.300000012, 'max_bin': 256, 'max_cat_threshold': 64, 'max_cat_to_onehot': 4, 'max_delta_step': 0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 1, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 100, 'n_jobs': 0, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': 0}\n",
      "\n",
      "model_name>>> xgb_random\n",
      "start time>>> 2022-11-07 16:24:37.535272\n",
      "end time>>>> 2022-11-07 16:24:47.367159\n",
      "dur 9.831887\n",
      "params {'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': 0, 'gpu_id': -1, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.1, 'max_bin': 256, 'max_cat_threshold': 64, 'max_cat_to_onehot': 4, 'max_delta_step': 0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 8, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 500, 'n_jobs': 0, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': 0}\n",
      "\n",
      "test and store scores.. xgb_random\n",
      "{'objective': 'reg:squarederror', 'base_score': 0.5, 'booster': 'gbtree', 'callbacks': None, 'colsample_bylevel': 1, 'colsample_bynode': 1, 'colsample_bytree': 1, 'early_stopping_rounds': None, 'enable_categorical': False, 'eval_metric': None, 'feature_types': None, 'gamma': 0, 'gpu_id': -1, 'grow_policy': 'depthwise', 'importance_type': None, 'interaction_constraints': '', 'learning_rate': 0.1, 'max_bin': 256, 'max_cat_threshold': 64, 'max_cat_to_onehot': 4, 'max_delta_step': 0, 'max_depth': 6, 'max_leaves': 0, 'min_child_weight': 8, 'missing': nan, 'monotone_constraints': '()', 'n_estimators': 500, 'n_jobs': 0, 'num_parallel_tree': 1, 'predictor': 'auto', 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 1, 'sampling_method': 'uniform', 'scale_pos_weight': 1, 'subsample': 1, 'tree_method': 'exact', 'validate_parameters': 1, 'verbosity': 0}\n",
      "\n",
      "model_name>>> lgbm_base\n",
      "start time>>> 2022-11-07 16:24:47.373964\n",
      "end time>>>> 2022-11-07 16:24:48.263103\n",
      "dur 0.889139\n",
      "\n",
      "test and store scores.. lgbm_base\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 1.0, 'importance_type': 'split', 'learning_rate': 0.1, 'max_depth': -1, 'min_child_samples': 20, 'min_child_weight': 0.001, 'min_split_gain': 0.0, 'n_estimators': 100, 'n_jobs': -1, 'num_leaves': 31, 'objective': None, 'random_state': 42, 'reg_alpha': 0.0, 'reg_lambda': 0.0, 'silent': 'warn', 'subsample': 1.0, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "model_name>>> lgbm_random\n",
      "start time>>> 2022-11-07 16:24:48.268591\n",
      "end time>>>> 2022-11-07 16:24:53.798224\n",
      "dur 5.529633\n",
      "params {'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.4, 'importance_type': 'split', 'learning_rate': 0.2, 'max_depth': 6, 'min_child_samples': 10, 'min_child_weight': 10.0, 'min_split_gain': 0.0, 'n_estimators': 400, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 50, 'silent': 'warn', 'subsample': 0.3, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "test and store scores.. lgbm_random\n",
      "{'boosting_type': 'gbdt', 'class_weight': None, 'colsample_bytree': 0.4, 'importance_type': 'split', 'learning_rate': 0.2, 'max_depth': 6, 'min_child_samples': 10, 'min_child_weight': 10.0, 'min_split_gain': 0.0, 'n_estimators': 400, 'n_jobs': -1, 'num_leaves': 20, 'objective': None, 'random_state': 42, 'reg_alpha': 0, 'reg_lambda': 50, 'silent': 'warn', 'subsample': 0.3, 'subsample_for_bin': 200000, 'subsample_freq': 0}\n",
      "\n",
      "model_name>>> catboost_base\n",
      "start time>>> 2022-11-07 16:24:53.804678\n",
      "end time>>>> 2022-11-07 16:24:55.006696\n",
      "dur 1.202018\n",
      "\n",
      "test and store scores.. catboost_base\n",
      "{'loss_function': 'RMSE', 'verbose': False, 'random_state': 42}\n",
      "\n",
      "model_name>>> catboost_random\n",
      "start time>>> 2022-11-07 16:24:55.010718\n"
     ]
    }
   ],
   "source": [
    "# 1. get train_val and test datas\n",
    "data_obj = Data(TRAIN_PATH, TEST_PATH)\n",
    "X_train_val, y_train_val = data_obj.get_xy_trainval()\n",
    "X_test, y_test = data_obj.get_xy_test()\n",
    "\n",
    "# 2. init the trainTest object and insert datas\n",
    "tt = TrainTest(X_train_val, y_train_val,X_test, y_test)\n",
    "\n",
    "# 3. init model object\n",
    "RF = Model(rf_name, rf_base, rf_random_grid)\n",
    "XGB = Model(xgb_name, xgb_base, xgb_random_grid)\n",
    "LGBM = Model(lgbm_name, lgbm_base, lgbm_random_grid)\n",
    "CB = Model(catboost_name, catboost_base, catboost_random_grid)\n",
    "\n",
    "# 4. append all base and random version of each models\n",
    "model_objects = [RF, XGB, LGBM, CB]\n",
    "all_base_random = []\n",
    "\n",
    "for model in model_objects:\n",
    "    base_name, base_model = model.get_base_name(), model.get_base_model()\n",
    "    random_name, random_model = model.get_random_name(), model.get_random_model()\n",
    "    all_base_random.extend([(base_name, base_model), (random_name, random_model)])\n",
    "\n",
    "\n",
    "# 5. train, test, shap all models\n",
    "tt.train_test_models(all_base_random)\n",
    "scores_df = tt.get_scores_df()\n",
    "print(scores_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31c700e-c8b3-4f99-baaa-db3421b3743f",
   "metadata": {},
   "source": [
    "# 8. Save Best Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037188d8-2228-4c2b-8be8-0db561e46e96",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_one = tt.get_bestN_random_models(1)\n",
    "print(\"best=\", best_one[0][0])\n",
    "\n",
    "cb_best = best_one[0][0]\n",
    "\n",
    "# save the best model\n",
    "filename = 'cb_best.sav'\n",
    "\n",
    "# try load\n",
    "loaded_model_name = \"cb_best\"\n",
    "loaded_model = pickle.load(open(filename, 'rb'))\n",
    "tt.train_and_store_score(loaded_model, loaded_model_name)\n",
    "tt.test_and_store_score(loaded_model, loaded_model_name)\n",
    "\n",
    "# confirm correct score\n",
    "scores_df = tt.get_scores_df()\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0593e42-f2fd-4804-93b1-69dbb6f1c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df.to_csv('scores.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7593ad-b0d4-4377-96c0-933de115a386",
   "metadata": {},
   "source": [
    "# 9.Tuning Best (Catboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01015e19-d32d-4fb1-904b-c395c98f0d78",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "cb_reg_grid = {\n",
    "        'depth': [9,10,11],\n",
    "        'learning_rate' : [0.02, 0.03, 0.04],\n",
    "        'l2_leaf_reg': [1, 2],\n",
    "        'n_estimators' : [400, 500, 600],\n",
    "        'verbose': [False],\n",
    "        'loss_function': ['RMSE'],\n",
    "        }\n",
    "s\n",
    "print(CB.get_base_name())\n",
    "\n",
    "cb_gridsearch, cb_grid_name = CB.set_get_reg_grid(cb_reg_grid), CB.get_grid_name()\n",
    "cb_best_grid = tt.train_and_store_score(cb_gridsearch, cb_grid_name)\n",
    "tt.test_and_store_score(cb_best_grid, cb_grid_name)\n",
    "tt.explainerObj.plot(cb_best_grid, cb_grid_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90bef31-d5a7-4c3c-b104-0118f2d38b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_df = tt.get_scores_df()\n",
    "print(scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a98fee-1d3f-4337-8aec-edc449194df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# catboost_best_random = {'learning_rate': 0.03, \n",
    "#                       'depth': 6,\n",
    "#                       'l2_leaf_reg': 1,\n",
    "#                       'loss_function': 'RMSE',\n",
    "#                       'verbose': False,\n",
    "#                       'n_estimators': 1000}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50df738-dda6-44e3-af60-dcf754cc1ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
