{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba628875-88b7-4b70-bbaa-4665018d58af",
   "metadata": {},
   "source": [
    "Use the final csv files for train and test and use various models to train and score them"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3470fa-5ba9-4de4-8bbe-c6562636c324",
   "metadata": {},
   "source": [
    "# 1. Import and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae8eec3d-cd80-40fd-8f52-fc1c339babbc",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# models\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# uncomment to install the three models below >>>>>\n",
    "# !pip3 install catboost\n",
    "# !pip install lightgbm\n",
    "# !pip3 install xgboost\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "# processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# parameters search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "\n",
    "# scoring\n",
    "import math\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "\n",
    "# viz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# explain\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "972dc900-b2a2-4bbf-8380-36c7b7838610",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"../Data/microclimate_model/Combined/three_bldgs_dropped.csv\"\n",
    "TEST_PATH = \"../Data/microclimate_model/Combined/three_bldgs_J9_dropped.csv\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135fd10a-6629-44c3-b201-175f96bdc824",
   "metadata": {},
   "source": [
    "# 2. Data Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eddd56cb-10f9-42c0-ab23-7cdc737bdb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "This class encapsulates the datas that we will need for training and testing.\n",
    "It only contains getters for the train and test data\n",
    "\"\"\"\n",
    "class Data(object):\n",
    "    def __init__(self, train_path, test_path):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "            train_path (str) : The file path for the trainning csv file. \n",
    "            test_path (str) : The path for the test csv file. \n",
    "        \n",
    "        Both train and test datas have 16 columns with buildings already encoded\n",
    "        \"\"\"\n",
    "        dropped_cols = ['CHWTON/SQM', \"bldgname_ISTB 4\"]\n",
    "        # - Train and validation data\n",
    "        self.train_val_df = pd.read_csv(train_path, index_col=0)\n",
    "        self.X_train_val = self.train_val_df.drop(columns=dropped_cols)\n",
    "        self.y_train_val = self.train_val_df['CHWTON/SQM']  \n",
    "        \n",
    "        # - Test data\n",
    "        self.test_df = pd.read_csv(test_path, index_col=0)\n",
    "        self.X_test = self.test_df.drop(columns=dropped_cols)\n",
    "        self.y_test = self.test_df['CHWTON/SQM'] \n",
    "        \n",
    "    \n",
    "    def get_xy_trainval(self):\n",
    "        \"\"\"\n",
    "        Return the X and y for training data which we can split to train and validation data later.\n",
    "        \"\"\"\n",
    "        return self.X_train_val, self.y_train_val\n",
    "    \n",
    "    def get_xy_test(self):\n",
    "        \"\"\"\n",
    "        Return the X and y for June 9th test data\n",
    "        \"\"\"\n",
    "        return self.X_test, self.y_test\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d18c5452-d235-436e-86a5-d2f58caa364d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class contains X_test datas that will be used to plot the explainer.\n",
    "It will plot the explainer using shap library. make sure Shap is already imported\n",
    "\"\"\"\n",
    "class Explainer(object):\n",
    "    \n",
    "    def __init__(self, X_test):\n",
    "        self.X_test = X_test\n",
    "        \n",
    "        # fig index\n",
    "        self.fig_id = 0\n",
    "        \n",
    "        \n",
    "    def plot(self, model, model_name):\n",
    "        # 1. init explainer\n",
    "        explainer = shap.TreeExplainer(\n",
    "            model=model,\n",
    "            data=None,\n",
    "            model_output='raw',\n",
    "            feature_perturbation='tree_path_dependent'\n",
    "        )\n",
    "        \n",
    "        # 2. get shap values from explainer\n",
    "        shap_values = explainer.shap_values(self.X_test)\n",
    "        \n",
    "        # 3. single sample plot\n",
    "        # shap.initjs()\n",
    "        # i = 7\n",
    "        # shap.force_plot(explainer.expected_value, shap_values[i,:], X_test.iloc[i,:], matplotlib = True)\n",
    "        # plt.title(model_name)\n",
    "\n",
    "        # 4. summary plot\n",
    "        plt.figure(self.fig_id)\n",
    "        \n",
    "        # increment fig index for next plot\n",
    "        self.fig_id += 1\n",
    "        \n",
    "        shap.summary_plot(shap_values, self.X_test, plot_size = [8,6], show = False)\n",
    "        plt.title(model_name)\n",
    "        plt.gcf().axes[-1].set_aspect(10)\n",
    "        plt.gcf().axes[-1].set_box_aspect(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfcd0b9-98f3-4638-8038-9fe520d7fa8e",
   "metadata": {},
   "source": [
    "# 3. Train Test Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "164d8a7e-fa55-4c2e-9579-b71fc42d4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class encapsulates the the training and testing process.\n",
    "It stores the train and test datas that's already split to X and y\n",
    "\"\"\"\n",
    "class TrainTest(object):\n",
    "    def __init__(self, X_train_val, y_train_val, X_test, y_test):\n",
    "        # - scores_df to display the scores for all our models\n",
    "        self.columns=['model','r2_val', 'r2_test', 'rmse_test','mbe_test']\n",
    "        self.scores_df= pd.DataFrame(columns=self.columns)\n",
    "        \n",
    "        # - train and test data\n",
    "        self.X_train_val = X_train_val\n",
    "        self.y_train_val = y_train_val\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "        \n",
    "        self.explainerObj = Explainer(self.X_test)\n",
    "    \n",
    "    def get_scores_df(self):\n",
    "        return self.scores_df\n",
    "    \n",
    "    \n",
    "    def train_and_store_score(self, model, model_name):\n",
    "        \"\"\"\n",
    "        This function will train the model given as parameter using the training data. It will compute the r2 validation score\n",
    "        and append this as a new row to scores_df.\n",
    "        \n",
    "        Parameters:\n",
    "            model (regressor model) : The model object that will be trained and used in validation.\n",
    "                It can be RF, XGB, LGBM, or catboost regressor\n",
    "                \n",
    "            model_name (str) : the name of the model displayed in scores_df\n",
    "            \n",
    "        RetursnL\n",
    "            model : this is relevant if we are doing randomized search. it will return the best model\n",
    "        \"\"\"\n",
    "        # 1. Train-Val Split\n",
    "        X_train, X_val, y_train, y_val = train_test_split(self.X_train_val, \n",
    "                                                          self.y_train_val, \n",
    "                                                          test_size=0.3, \n",
    "                                                          random_state=20)\n",
    "\n",
    "        # 2. fit model that already has parameters\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "        # - Get best params if it's a random or grid search\n",
    "        if(\"random\" in model_name) or (\"grid\" in model_name):\n",
    "            # print(model.best_estimator_.get_params())\n",
    "            model = model.best_estimator_\n",
    "            \n",
    "        \n",
    "        \n",
    "        # 3. get validation R2 score\n",
    "        val_r2 = model.score(X_val, y_val)\n",
    "        \n",
    "        # 4. store score\n",
    "        new_row_data = {'model':model_name, \"r2_val\":val_r2, \"r2_test\":0, 'rmse_test':0, 'mbe_test':0}\n",
    "        new_row = pd.DataFrame.from_records([new_row_data])\n",
    "        self.scores_df = pd.concat([self.scores_df, new_row])\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def get_MBE(self, y_true, y_pred):\n",
    "        '''\n",
    "        Parameters:\n",
    "            y_true (array): Array of observed values\n",
    "            y_pred (array): Array of prediction values\n",
    "\n",
    "        Returns:\n",
    "            mbe (float): Bias score\n",
    "        '''\n",
    "        y_true = np.array(y_true)\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = y_true.reshape(len(y_true),1)\n",
    "        y_pred = y_pred.reshape(len(y_pred),1)   \n",
    "        diff = (y_pred-y_true)\n",
    "        mbe = diff.mean()\n",
    "        return mbe\n",
    "\n",
    "        \n",
    "    def test_and_store_score(self, model, model_name):\n",
    "        \"\"\"\n",
    "        This function will use the given trained model to compute the y_pred using the X_test data.\n",
    "        It will then compute the mbe, r2, and rmse result and insert it to scores_df. \n",
    "        \n",
    "        Parameters:\n",
    "            model (regressor model): The model that has been trained and will be used to predict y using the test data\n",
    "                It can be RF, XGB, LGBM, or catboost regressor\n",
    "            model_name (string): the name of the model displayed in scores_df\n",
    "        \n",
    "        \"\"\"\n",
    "        # 1. Get prediction for the test data\n",
    "        y_pred = model.predict(self.X_test)\n",
    "        \n",
    "        # 2. get the three scores\n",
    "        r2 = r2_score(self.y_test, y_pred)\n",
    "        rmse = math.sqrt(mean_squared_error(self.y_test, y_pred))\n",
    "        mbe = self.get_MBE(self.y_test, y_pred)\n",
    "        \n",
    "        # 3. update scores_df with the 3 scores above\n",
    "        row_to_update = self.scores_df[\"model\"] == model_name\n",
    "        col_to_update = ['r2_test','rmse_test', 'mbe_test']\n",
    "        self.scores_df.loc[row_to_update, col_to_update] = [r2, rmse, mbe]\n",
    "        \n",
    "        # print(\"r2:\", r2)\n",
    "        # print(\"rmse:\", rmse)\n",
    "        # print(\"mbe:\", mbe)\n",
    "        \n",
    "    def train_test_plot_models(self, model_list):\n",
    "        for model_name, model in model_list:\n",
    "            \n",
    "            # - need to do this in case its randomized search\n",
    "            best_model = self.train_and_store_score(model, model_name)\n",
    "            self.test_and_store_score(best_model, model_name)\n",
    "            self.explainerObj.plot(best_model, model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1be3d3-67b2-475d-a58f-49edb75a3ce4",
   "metadata": {},
   "source": [
    "# 4. Random Forest Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a761d176-1141-44c7-beb4-eb9a432211c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "class Model():\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_base_model(self):\n",
    "        return self.base_name, self.base\n",
    "    \n",
    "    @abstractmethod\n",
    "    def get_random_model(self):\n",
    "        return self.random_name, self.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13aa2bea-1087-46a6-bfef-a39dfd52eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class contains the Random forest base model and the tuned model using randomized search cv.\n",
    "\"\"\"\n",
    "class RandomForest(Model):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # A. base model\n",
    "        self.base_name = \"rf_base\"\n",
    "        self.base = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "        \n",
    "        \n",
    "        # B. Randomized tuned model \n",
    "        # 1. Number of trees in random forest\n",
    "        # [50, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n",
    "        n_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 10)]\n",
    "\n",
    "        # 2. Maximum number of levels in tree\n",
    "        # [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
    "        max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "        max_depth.append(None)\n",
    "\n",
    "        # 3. Minimum number of samples required to split a node\n",
    "        min_samples_split = [2, 5, 10]\n",
    "\n",
    "        # 4. Minimum number of samples required at each leaf node\n",
    "        min_samples_leaf = [ 1, 2, 4]\n",
    "\n",
    "        # 5. Method of selecting samples for training each tree\n",
    "        bootstrap = [True]\n",
    "\n",
    "        # 6. Number of features to consider at every split\n",
    "        max_features = [\"sqrt\", \"log2\", None, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "        \n",
    "        # 7.\n",
    "        criterion = ['squared_error', 'absolute_error', 'poisson']\n",
    "        \n",
    "        # Create the random grid\n",
    "        random_grid = {'n_estimators': n_estimators,\n",
    "                       'max_features': max_features,\n",
    "                       'max_depth': max_depth,\n",
    "                       'criterion': criterion,\n",
    "                       'min_samples_split': min_samples_split,\n",
    "                       'min_samples_leaf': min_samples_leaf,\n",
    "                       'bootstrap': bootstrap}\n",
    "        \n",
    "# [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110]\n",
    "        self.random_name = \"rf_random\"\n",
    "        self.random = RandomizedSearchCV(\n",
    "                            estimator = self.base,\n",
    "                            param_distributions = random_grid,\n",
    "                            n_iter = 20,\n",
    "                            cv = 5,\n",
    "                            verbose = 2,\n",
    "                            scoring ='r2',\n",
    "                            random_state = 42,\n",
    "                            n_jobs = -1)\n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f36b55-1521-45bb-8e7f-fac6648d857b",
   "metadata": {},
   "source": [
    "# 5. XGB class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "60170e9a-34e9-43d7-9f5f-b5b33d4a2406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class contains the XGB base model and the tuned model using randomized search cv.\n",
    "It only contain getters that return the model and the model name for each of the models\n",
    "\"\"\"\n",
    "class XGB(Model):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # A. base model\n",
    "        self.base_name = \"xgb_base\"\n",
    "        self.base = XGBRegressor(n_estimators = 100, random_state = 42)\n",
    "        \n",
    "        \n",
    "        # B. Randomized tuned model \n",
    "        random_grid = {\n",
    "            'learning_rate' : [0.1, 0,2 ,0.3, 0.4],\n",
    "            'n_estimators':[ 100, 250, 500, 1000],\n",
    "            'min_child_weight':[1, 2, 4, 5, 8], \n",
    "            'max_depth': [4,6,7,8],\n",
    "            'colsample_bytree' : [ 0.3, 0.4, 0.5 , 0.7, 1 ],\n",
    "            'booster': ['gbtree', 'gblinear']\n",
    "        }\n",
    "        \n",
    "        self.random_name = \"xgb_random\"\n",
    "        self.random = RandomizedSearchCV(\n",
    "                            estimator = self.base,\n",
    "                            param_distributions = random_grid,\n",
    "                            n_iter = 20,\n",
    "                            cv = 5,\n",
    "                            verbose = 0,\n",
    "                            scoring ='r2',\n",
    "                            random_state = 42,\n",
    "                            n_jobs = -1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72248026-1887-4aac-bf10-8686163ff4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class contains the XGB base model and the tuned model using randomized search cv.\n",
    "It only contain getters that return the model and the model name for each of the models\n",
    "\"\"\"\n",
    "class LGBM(Model):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # A. base model\n",
    "        self.base_name = \"lgbm_base\"\n",
    "        self.base = LGBMRegressor(random_state = 42)\n",
    "        \n",
    "        # B. Randomized tuned model \n",
    "        random_grid = {\n",
    "                'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "                'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000, 1500, 2000],\n",
    "                'num_leaves': [10,15,20,30,40], \n",
    "                'min_child_samples': [10,20,40,50,100],\n",
    "                'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3],\n",
    "                'subsample': [0.1, 0.3, 0.8, 1.0], \n",
    "                'max_depth': [-1, 1, 2, 3, 4, 5, 6, 7],\n",
    "                'colsample_bytree': [0.4, 0.5, 0.6, 1.0],\n",
    "                'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n",
    "                'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}\n",
    "        \n",
    "        self.random_name = \"lgbm_random\"\n",
    "        self.random = RandomizedSearchCV(\n",
    "                            estimator = self.base,\n",
    "                            param_distributions = random_grid,\n",
    "                            n_iter = 20,\n",
    "                            cv = 5,\n",
    "                            verbose = 0,\n",
    "                            scoring ='r2',\n",
    "                            random_state = 42,\n",
    "                            n_jobs = -1)\n",
    "        \n",
    "        \n",
    "        # {'boosting_type': 'gbdt',\n",
    "        #  'class_weight': None, \n",
    "        #  'colsample_bytree': 1.0,\n",
    "        #  'importance_type': 'split', \n",
    "        #  'learning_rate': 0.1,\n",
    "        #  'max_depth': -1,\n",
    "        #  'min_child_samples': 20, \n",
    "        #  'min_child_weight': 0.001, \n",
    "        #  'min_split_gain': 0.0,\n",
    "        #  'n_estimators': 100,\n",
    "        #  'n_jobs': -1,\n",
    "        #  'num_leaves': 31,\n",
    "        #  'objective': None, \n",
    "        #  'random_state': 42,\n",
    "        #  'reg_alpha': 0.0, \n",
    "        #  'reg_lambda': 0.0, \n",
    "        #  'silent': 'warn',\n",
    "        #  'subsample': 1.0, \n",
    "        #  'subsample_for_bin': 200000, \n",
    "        #  'subsample_freq': 0}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a347538-80f5-4a1a-ad42-350c9817f273",
   "metadata": {},
   "source": [
    "# 7. Catboost Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bed3de11-ad56-42fa-a633-c4ed283d1623",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This class contains the Catboost base model and the tuned model using randomized search cv.\n",
    "It only contain getters that return the model and the model name for each of the models\n",
    "\"\"\"\n",
    "class Catboost(Model):\n",
    "    def __init__(self):\n",
    "        \n",
    "        # A. base model\n",
    "        self.base_name = \"catboost_base\"\n",
    "        self.base = CatBoostRegressor(random_state = 42, verbose=False)\n",
    "        \n",
    "        \n",
    "        # B. Randomized tuned model \n",
    "        random_grid = {\n",
    "                'learning_rate' : [0.01, 0.02, 0.03, 0.04, 0.05, 0.08, 0.1, 0.2, 0.3, 0.4],\n",
    "                'n_estimators' : [100, 200, 300, 400, 500, 600, 800, 1000],\n",
    "                'depth': [4, 6, 10],\n",
    "                'l2_leaf_reg': [1, 3, 5, 7, 9]}\n",
    "        \n",
    "        self.random_name = \"catboost_random\"\n",
    "        self.random = RandomizedSearchCV(\n",
    "                            estimator = self.base,\n",
    "                            param_distributions = random_grid,\n",
    "                            n_iter = 20,\n",
    "                            cv = 5,\n",
    "                            verbose = 0,\n",
    "                            scoring ='r2',\n",
    "                            random_state = 42,\n",
    "                            n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "25054a46-840e-45bd-ac25-b699229ae2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  To train, test, and plot individually use this example:\n",
    "# rf_random_best = tt.train_and_store_score(rf_random, rf_random_name)\n",
    "# tt.test_and_store_score(rf_random_best, rf_random_name)\n",
    "# tt.explainerObj.plot(rf_random_best, rf_random_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "242440bd-5d08-4a1e-b904-67a181794615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 1. get train_val and test datas\n",
    "    data_obj = Data(TRAIN_PATH, TEST_PATH)\n",
    "    X_train_val, y_train_val = data_obj.get_xy_trainval()\n",
    "    X_test, y_test = data_obj.get_xy_test()\n",
    "\n",
    "    # 2. init the trainTest object and insert datas\n",
    "    tt = TrainTest(X_train_val, y_train_val,X_test, y_test)\n",
    "\n",
    "\n",
    "    # 3. init model object\n",
    "    rf = RandomForest()\n",
    "    xgb = XGB()\n",
    "    lgbm = LGBM()\n",
    "    catboost = Catboost()\n",
    "\n",
    "    # 4. append all base and random version of each models\n",
    "    model_objects = [rf, xgb, lgbm, catboost]\n",
    "    all_base_random = []\n",
    "\n",
    "    for model in model_objects:\n",
    "        base_name, base_model = model.get_base_model()\n",
    "        random_name, random_model = model.get_random_model()\n",
    "        all_base_random.extend([(base_name, base_model), (random_name, random_model)])\n",
    "\n",
    "\n",
    "    # 5. train, test, shap all models\n",
    "    tt.train_test_plot_models(all_base_random)\n",
    "    scores_df = tt.get_scores_df()\n",
    "    print(scores_df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229264e1-0ab8-4dde-a2df-2750dd1d970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n",
      "ntree_limit is deprecated, use `iteration_range` or model slicing instead.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101e4757-3a4d-45aa-85de-b3fe031fc4c6",
   "metadata": {},
   "source": [
    "## Note:\n",
    "1. drop istb_4 column since its redundant. increased the RF test result.\n",
    "2. changed the radnomised search cv for RF. now the result is quite good\n",
    "3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8de70af-4c6b-4832-95c6-bbab58a61c97",
   "metadata": {},
   "source": [
    "# 1. Config type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8cd028-05e1-499a-9f4b-60fb46e6701f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:root] *",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
